"""
ë¬¸ì„œ ìë™ ì—…ë°ì´íŠ¸ ì‹œìŠ¤í…œ
íŒŒì¼ ë³€ê²½ ê°ì§€ ë° ì‹¤ì‹œê°„ ë¬¸ì„œ ì¬ìƒì„±
"""
import hashlib
import json
import logging
import os
import threading
import time
from datetime import datetime
from pathlib import Path
from typing import Any, Callable, Dict, List

try:
    from watchdog.events import FileSystemEventHandler
    from watchdog.observers import Observer
    WATCHDOG_AVAILABLE = True
except ImportError:
    WATCHDOG_AVAILABLE = False
    print("âš ï¸ watchdog ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. pip install watchdogë¡œ ì„¤ì¹˜í•˜ì„¸ìš”.")


class DocumentAutoUpdater:
    """ë¬¸ì„œ ìë™ ì—…ë°ì´íŠ¸ ê´€ë¦¬ì"""

    def __init__(self, project_root: Path = None):
        if project_root is None:
            project_root = Path(__file__).parent.parent.parent

        self.project_root = Path(project_root)
        self.docs_root = self.project_root / "docs"
        self.logger = logging.getLogger(__name__)

        # ì„¤ì • ë¡œë“œ
        self.config = self._load_config()

        # ìƒíƒœ ê´€ë¦¬
        self.last_update = None
        self.update_count = 0
        self.file_hashes = {}
        self.is_running = False
        self.observer = None

        # ì—…ë°ì´íŠ¸ ì½œë°±
        self.update_callbacks: List[Callable] = []

    def _load_config(self) -> Dict[str, Any]:
        """ë¬¸ì„œ ì„¤ì • íŒŒì¼ ë¡œë“œ"""
        config_path = self.docs_root / "docs_config.json"

        if config_path.exists():
            try:
                with open(config_path, 'r', encoding='utf-8') as f:
                    return json.load(f)
            except Exception as e:
                self.logger.warning(f"ì„¤ì • íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {e}")

        # ê¸°ë³¸ ì„¤ì •
        return {
            "auto_update": {
                "enabled": True,
                "watch_directories": ["src/", "templates/"],
                "watch_files": ["*.py", "*.html", "*.md"],
                "exclude_patterns": ["__pycache__", "*.pyc", ".git", "*.log"],
                "debounce_seconds": 2
            },
            "generation_settings": {
                "formats": ["markdown", "html", "json"],
                "include_timestamps": True
            }
        }

    def add_update_callback(self, callback: Callable):
        """ì—…ë°ì´íŠ¸ ì½œë°± í•¨ìˆ˜ ë“±ë¡"""
        self.update_callbacks.append(callback)

    def _calculate_file_hash(self, file_path: Path) -> str:
        """íŒŒì¼ í•´ì‹œ ê³„ì‚°"""
        try:
            with open(file_path, 'rb') as f:
                return hashlib.md5(f.read()).hexdigest()
        except Exception:
            return ""

    def _should_trigger_update(self, file_path: Path) -> bool:
        """íŒŒì¼ ë³€ê²½ì´ ì—…ë°ì´íŠ¸ë¥¼ íŠ¸ë¦¬ê±°í•´ì•¼ í•˜ëŠ”ì§€ í™•ì¸"""
        # ì œì™¸ íŒ¨í„´ í™•ì¸
        exclude_patterns = self.config.get(
            "auto_update", {}).get("exclude_patterns", [])
        for pattern in exclude_patterns:
            if pattern.replace("*", "") in str(file_path):
                return False

        # ê°ì‹œ ëŒ€ìƒ íŒŒì¼ í™•ì¸
        watch_files = self.config.get(
            "auto_update", {}).get("watch_files", ["*"])
        file_extension = file_path.suffix

        for pattern in watch_files:
            if pattern == "*" or pattern.endswith(file_extension):
                return True

        return False

    def _trigger_documentation_update(self, changed_file: Path = None):
        """ë¬¸ì„œ ì—…ë°ì´íŠ¸ íŠ¸ë¦¬ê±°"""
        try:
            self.logger.info(f"ë¬¸ì„œ ì—…ë°ì´íŠ¸ ì‹œì‘ - ë³€ê²½ íŒŒì¼: {changed_file}")
            print(f"ğŸ”„ ë¬¸ì„œ ìë™ ì—…ë°ì´íŠ¸ ì‹œì‘ (ë³€ê²½: {changed_file})")

            # ë””ë°”ìš´ì‹± (ì—°ì† ë³€ê²½ ë°©ì§€)
            debounce = self.config.get(
                "auto_update", {}).get("debounce_seconds", 2)
            time.sleep(debounce)

            # ë¬¸ì„œ ìƒì„± ì‹¤í–‰
            from src.docs import generate_all_documentation

            start_time = time.time()
            result = generate_all_documentation()

            if result and 'generated_docs' in result:
                execution_time = time.time() - start_time
                self.last_update = datetime.now()
                self.update_count += 1

                # ìƒíƒœ ì •ë³´
                generated_docs = result['generated_docs']
                scan_result = result.get('scan_result', {})
                statistics = scan_result.get('statistics', {})

                print(f"âœ… ìë™ ì—…ë°ì´íŠ¸ ì™„ë£Œ ({execution_time:.2f}ì´ˆ)")
                print(f"  ğŸ“„ ìƒì„±ëœ ë¬¸ì„œ: {len(generated_docs)}ê°œ")
                print(
                    f"  ğŸ” ìŠ¤ìº”ëœ ì—”ë“œí¬ì¸íŠ¸: {statistics.get('total_endpoints', 0)}ê°œ")

                # ì½œë°± ì‹¤í–‰
                for callback in self.update_callbacks:
                    try:
                        callback(result)
                    except Exception as e:
                        self.logger.error(f"ì½œë°± ì‹¤í–‰ ì‹¤íŒ¨: {e}")

                # ì—…ë°ì´íŠ¸ ë¡œê·¸ ì €ì¥
                self._log_update(changed_file, result)

            else:
                print("âŒ ìë™ ì—…ë°ì´íŠ¸ ì‹¤íŒ¨")
                self.logger.error("ë¬¸ì„œ ìƒì„± ì‹¤íŒ¨")

        except Exception as e:
            print(f"âŒ ìë™ ì—…ë°ì´íŠ¸ ì˜¤ë¥˜: {e}")
            self.logger.error(f"ìë™ ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {e}")

    def _log_update(self, changed_file: Path, result: Dict[str, Any]):
        """ì—…ë°ì´íŠ¸ ë¡œê·¸ ê¸°ë¡"""
        log_path = self.docs_root / "update_log.json"

        log_entry = {
            "timestamp": self.last_update.isoformat(),
            "trigger_file": str(changed_file) if changed_file else None,
            "update_count": self.update_count,
            "generated_files": list(result.get('generated_docs', {}).keys()),
            "statistics": result.get('scan_result', {}).get('statistics', {})
        }

        # ê¸°ì¡´ ë¡œê·¸ ë¡œë“œ
        logs = []
        if log_path.exists():
            try:
                with open(log_path, 'r', encoding='utf-8') as f:
                    logs = json.load(f)
            except Exception:
                pass

        # ìƒˆ ë¡œê·¸ ì¶”ê°€ (ìµœëŒ€ 50ê°œ ìœ ì§€)
        logs.append(log_entry)
        if len(logs) > 50:
            logs = logs[-50:]

        # ë¡œê·¸ ì €ì¥
        try:
            with open(log_path, 'w', encoding='utf-8') as f:
                json.dump(logs, f, indent=2, ensure_ascii=False)
        except Exception as e:
            self.logger.error(f"ì—…ë°ì´íŠ¸ ë¡œê·¸ ì €ì¥ ì‹¤íŒ¨: {e}")


class DocumentFileHandler(FileSystemEventHandler):
    """íŒŒì¼ ì‹œìŠ¤í…œ ì´ë²¤íŠ¸ í•¸ë“¤ëŸ¬"""

    def __init__(self, updater: DocumentAutoUpdater):
        self.updater = updater
        self.logger = logging.getLogger(__name__)

    def on_modified(self, event):
        if not event.is_directory:
            file_path = Path(event.src_path)
            if self.updater._should_trigger_update(file_path):
                print(f"ğŸ“ íŒŒì¼ ë³€ê²½ ê°ì§€: {file_path}")
                # ë³„ë„ ìŠ¤ë ˆë“œì—ì„œ ì—…ë°ì´íŠ¸ ì‹¤í–‰ (ë¸”ë¡œí‚¹ ë°©ì§€)
                update_thread = threading.Thread(
                    target=self.updater._trigger_documentation_update,
                    args=(file_path,),
                    daemon=True
                )
                update_thread.start()


def start_auto_updater(project_root: Path = None) -> DocumentAutoUpdater:
    """ìë™ ì—…ë°ì´í„° ì‹œì‘ (í¸ì˜ í•¨ìˆ˜)"""
    if not WATCHDOG_AVAILABLE:
        print("âŒ watchdog ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ í•„ìš”í•©ë‹ˆë‹¤. pip install watchdog")
        return None

    updater = DocumentAutoUpdater(project_root)

    if not updater.config.get("auto_update", {}).get("enabled", True):
        print("âš ï¸ ìë™ ì—…ë°ì´íŠ¸ê°€ ë¹„í™œì„±í™”ë˜ì–´ ìˆìŠµë‹ˆë‹¤.")
        return updater

    try:
        # ê°ì‹œ ë””ë ‰í† ë¦¬ ì„¤ì •
        watch_dirs = updater.config.get("auto_update", {}).get(
            "watch_directories", ["src/"])

        # íŒŒì¼ ì‹œìŠ¤í…œ ê°ì‹œ ì‹œì‘
        event_handler = DocumentFileHandler(updater)
        observer = Observer()

        for watch_dir in watch_dirs:
            dir_path = updater.project_root / watch_dir
            if dir_path.exists():
                observer.schedule(event_handler, str(dir_path), recursive=True)
                print(f"ğŸ‘ï¸ ë””ë ‰í† ë¦¬ ê°ì‹œ ì‹œì‘: {dir_path}")

        observer.start()
        updater.observer = observer
        updater.is_running = True

        print("ğŸš€ ë¬¸ì„œ ìë™ ì—…ë°ì´í„° ì‹œì‘ë¨")
        print("ğŸ“ íŒŒì¼ ë³€ê²½ ì‹œ ìë™ìœ¼ë¡œ ë¬¸ì„œê°€ ì—…ë°ì´íŠ¸ë©ë‹ˆë‹¤.")

        return updater

    except Exception as e:
        print(f"âŒ ìë™ ì—…ë°ì´í„° ì‹œì‘ ì‹¤íŒ¨: {e}")
        return None


def stop_auto_updater(updater: DocumentAutoUpdater):
    """ìë™ ì—…ë°ì´í„° ì¤‘ì§€"""
    if updater and updater.observer:
        updater.observer.stop()
        updater.observer.join()
        updater.is_running = False
        print("â¹ï¸ ë¬¸ì„œ ìë™ ì—…ë°ì´í„° ì¤‘ì§€ë¨")


def manual_update_trigger():
    """ìˆ˜ë™ ë¬¸ì„œ ì—…ë°ì´íŠ¸ íŠ¸ë¦¬ê±° (í¸ì˜ í•¨ìˆ˜)"""
    updater = DocumentAutoUpdater()
    updater._trigger_documentation_update()
    return updater.last_update
